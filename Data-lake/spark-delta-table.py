from pyspark.sql import SparkSession
from delta import configure_spark_with_delta_pip
from pyspark.sql.functions import col
from pyspark.sql.types import StringType
from glob import glob
import os
import json
from delta.tables import DeltaTable

def main():
    builder = (
        SparkSession.builder.master("local[*]")
        .appName("Incremental Delta Table Load")
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        .config("spark.driver.memory", "10g")
        .config("spark.executor.memory", "6g")
        .config("spark.sql.debug.maxToStringFields", "1000")
    )
    spark = configure_spark_with_delta_pip(builder).getOrCreate()
    
    # Danh s√°ch c√°c path quan tr·ªçng
    delta_path = "/opt/airflow/Project-Feature-Store/Deltatable"  # Delta table

    checkpoint_file = "/opt/airflow/Project-Feature-Store/processed_files.json"  # Checkpoint file ƒë·ªÉ so s√°nh
    new_files_output= "/opt/airflow/Project-Feature-Store/new_files_output.json"  # l∆∞u tr·ªØ danh s√°ch file m·ªõi ph√°t hi·ªán

    version_tracker_file = "/opt/airflow/Project-Feature-Store/delta_version_tracker.json"  # File theo d√µi version
    
    # ========== ƒê·ªåC CHECKPOINT FILE ==========
    if os.path.exists(checkpoint_file):
        with open(checkpoint_file, 'r') as f:
            processed_files = set(json.load(f))
        print(f"üìã ƒê√£ c√≥ {len(processed_files)} file ƒë∆∞·ª£c x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥")
    else:
        processed_files = set()
        with open(checkpoint_file, 'w') as f:
            json.dump([], f)
        print("üìã T·∫°o m·ªõi file processed_files.json v√† ch∆∞a c√≥ file n√†o ƒë∆∞·ª£c x·ª≠ l√Ω")
    
    # ========== ƒê·ªåC new_files_output ==========

    if not os.path.exists(new_files_output): # N·∫øu ch∆∞a c√≥ file th√¨ t·∫°o m·ªõi
        with open(new_files_output, 'w') as f:
            json.dump([], f)
        print("üìã T·∫°o m·ªõi file new_files_output.json")
    else:    
        print("üìã ƒê√£ c√≥ s·∫µn new_files_output.json")

    # ========= ƒê·ªçc version tracker file ==========
    
    if os.path.exists(version_tracker_file):
        print("---- ƒê√£ c√≥ s·∫µn file version tracker-----")
    else:
        with open(version_tracker_file, 'w') as f:
            json.dump({}, f)
    
    # ========== L·∫§Y DANH S√ÅCH FILE M·ªöI ==========
    parquet_files = glob("/opt/airflow/NYC-data/*.parquet")
    parquet_files.sort()
    
    new_files = [f for f in parquet_files if f not in processed_files]
    
    if not new_files: #new_files r·ªóng
        print("=== Kh√¥ng c√≥ file m·ªõi n√†o c·∫ßn x·ª≠ l√Ω! ===")
        spark.stop()
        return
    
    #### N·∫øu t√¨m th·∫•y, b·∫Øt ƒë·∫ßu x·ª≠ l√Ω
    print(f"----T√¨m th·∫•y {len(new_files)} file m·ªõi c·∫ßn x·ª≠ l√Ω----")
    
    # ======== H√ÄM √âP KI·ªÇU V·ªÄ STRING ======
    def to_string(df):
        for field in df.schema.fields:
            df = df.withColumn(field.name, col(field.name).cast(StringType()))
        return df
    
    # H√†m √©p t√™n c√°c c·ªôt th√†nh lower case

    def lowercase_columns(df):
        for col_name in df.columns:
            df = df.withColumnRenamed(col_name, col_name.lower())
        return df
    
    # ========= L∆ØU VERSION TR∆Ø·ªöC KHI GHI =========
    
    if os.path.exists(delta_path) and os.listdir(delta_path): 
        delta_table = DeltaTable.forPath(spark, delta_path) #l·∫≠p ƒëelta table t·ª´ path
        version_before = delta_table.history(1).select("version").collect()[0][0] # l·∫•y version hi·ªán t·∫°i
        print(f"==== Delta Table hi·ªán t·∫°i ·ªü version {version_before} ====")
    else:
        version_before = -1
        print("=== V√¨ Delta Table ch∆∞a t·ªìn t·∫°i, n√™n version_before √©p b·∫±ng -1 ===")
    
    # ========= X·ª¨ L√ù T·ª™NG FILE M·ªöI V√Ä GHI V√ÄO DELTA TABLE =========

    print("=== B·∫Øt ƒë·∫ßu x·ª≠ l√Ω c√°c file m·ªõi v√† ghi v√†o Delta Table......")

    for idx, parquet in enumerate(new_files):
        name = os.path.splitext(os.path.basename(parquet))[0]
        print(f"-----{idx+1}/{len(new_files)}: B·∫Øt ƒë·∫ßu v·ªõi file {name} v√†o Deltatable-----")
    
        df = spark.read.parquet(parquet)

        df= lowercase_columns(df)
        df_fix = to_string(df)
    
        if version_before == -1 :
            print(f"===  B·∫Øt ƒë·∫ßu t·∫°o Delta Table v·ªõi CDF enabled .... ===")
            df_fix.write.format("delta") \
                .mode("overwrite") \
                .option("delta.enableChangeDataFeed", "true") \
                .save(delta_path)
            version_before = 0  # C·∫≠p nh·∫≠t version_before kh√°c v·ªõi -1 ƒë·ªÉ c√°c l·∫ßn sau v√†o nh√°nh else
            print(f"=== ‚úÖ Th√†nh c√¥ng t·∫°o Delta Table ===")
        else:
           
            df_fix.write.format("delta") \
                .mode("append") \
                .option("mergeSchema", "true") \
                .save(delta_path)
            print(f"=== ‚úÖ Append file {name} v√†o Delta Table ===")
        
        # C·∫¨P NH·∫¨T CHECKPOINT
        processed_files.add(parquet)
        with open(checkpoint_file, 'w') as f:
            json.dump(list(processed_files), f, indent=2)
        
        print(f"-----‚úÖ File {name} ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω, c√≤n l·∫°i {len(new_files) - (idx + 1)} file-----")
    
    print(f"==== Ho√†n th√†nh ghi {len(new_files)} file v√†o Delta Table====")

    # ====L∆∞u version sau khi ghi ======

    delta_table = DeltaTable.forPath(spark, delta_path)
    version_after = delta_table.history(1).select("version").collect()[0][0]

    print(f"==== Delta Table sau khi ghi c√≥ version l√†: {version_after} ====")
    
    with open(version_tracker_file, 'w') as f:
        json.dump({
            "last_processed_version": version_before,
            "current_version": version_after,
            "new_files_count": len(new_files),
            "has_new_data": True
        }, f, indent=2)

    # ====== C·∫≠p nh·∫≠p c√°c file m·ªõi v√†o new_files_output.json ======
    with open(new_files_output, 'w') as f:
        json.dump(new_files, f, indent=2)
    
    print("\n" + "="*60 + "T·ªîNG K·∫æT")
    
    df_final = spark.read.format("delta").load(delta_path)
    total_rows = df_final.count()
    print(f"‚úÖ T·ªïng s·ªë d√≤ng trong Delta Table: {total_rows}")
    print(f"‚úÖ S·ªë file m·ªõi v·ª´a th√™m: {len(new_files)}")
    print(f"‚úÖ Delta Table version: {version_before} ‚Üí {version_after}")
    
    print("\nüìã Sample data (10 d√≤ng):")
    df_final.show(10, truncate=False)
    
    spark.stop()
    print("----- Ho√†n t·∫•t!")

if __name__ == "__main__":
    main()
