from pyspark.sql import SparkSession
from delta import configure_spark_with_delta_pip
import os
import json

def main():
    # Spark Config 
    print("-----kh·ªüi t·∫°o Spark Session...------")
    
    builder = (
        SparkSession.builder
        .master("local[16]")
        .appName("MinIO to PostgreSQL")
        
        # Delta Lake configs
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        .config("spark.sql.debug.maxToStringFields", "1000")

        .config("spark.sql.shuffle.partitions", "16")  # b·∫±ng lu·ªìng CPU
        .config("spark.default.parallelism", "16")  # b·∫±ng lu·ªìng CPU

        .config("spark.executor.memory", "10g")
        .config("spark.driver.memory", "14g")
        .config("spark.driver.maxResultSize", "4g")
        .config("spark.memory.fraction", "0.8") # TƒÉng b·ªô nh·ªõ cho c√°c thao t√°c t√≠nh to√°n

        # OFF-HEAP MEMORY
        .config("spark.memory.offHeap.enabled", "true")
        .config("spark.memory.offHeap.size", "5g")  

        # B·ªï sung JARs cho Hadoop AWS v√† AWS SDK + postgreSQL ƒë·ªÉ ƒë·ªôc v√† ghi tr√™n S3 v√† PostgreSQL
        .config("spark.jars", "/opt/airflow/jars/hadoop-aws-3.3.4.jar,/opt/airflow/jars/aws-java-sdk-bundle-1.12.262.jar,/opt/airflow/jars/postgresql-42.7.1.jar")
    )
    
    spark = configure_spark_with_delta_pip(builder).getOrCreate()
    print("---‚úÖ Spark Session ƒë√£ kh·ªüi t·∫°o th√†nh c√¥ng!---")
    
    # ======= ƒê·ªçc file ======
    version_tracker_file = "/opt/airflow/Project-Feature-Store/delta_version_tracker.json"
    
    if not os.path.exists(version_tracker_file):
        print("------ Kh√¥ng c√≥ th√¥ng tin version tracker!------")
        spark.stop()
        return
    
    with open(version_tracker_file, 'r') as f:
        version_info = json.load(f)
    
    if not version_info.get("has_new_data", False):
        print("------ Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi!------")
        spark.stop()
        return
    
    last_version = version_info["last_processed_version"]
    current_version = version_info["current_version"]
    
    print(f"--- C√≥ {version_info['new_files_count']} file m·ªõi (version {last_version} ‚Üí {current_version})----")
    
    # ======= ƒê·ªåC D·ªÆ LI·ªÜU M·ªöI T·ª™ DELTA TABLE =======
    delta_path = "/opt/airflow/Project-Feature-Store/Deltatable"
    
    if last_version == -1:
        # L·∫ßn ƒë·∫ßu ti√™n, ƒë·ªçc to√†n b·ªô
        print("--- ƒê·ªçc to√†n b·ªô Delta Table l·∫ßn ƒë·∫ßu ---")
        df = spark.read.format("delta").load(delta_path)
    else:
        print(f"--- ƒê·ªçc incremental t·ª´ version {last_version + 1} ƒë·∫øn {current_version} ---")
        df_cdf = spark.read.format("delta") \
            .option("readChangeFeed", "true") \
            .option("startingVersion", last_version + 1) \
            .option("endingVersion", current_version) \
            .load(delta_path)
    # L·ªçc ch·ªâ l·∫•y c√°c b·∫£n ghi m·ªõi (inserts)
        df = df_cdf.filter("_change_type IN ('insert', 'update_postimage')") \
                   .drop("_change_type", "_commit_version", "_commit_timestamp")
    
    row_count = df.count()
    print(f"üìä T·ªïng s·ªë d√≤ng c·∫ßn upload: {row_count}")
    
    # =========C·∫§U H√åNH POSTGRESQL CONNECTION=========
    postgres_config = {
        "url": "jdbc:postgresql://172.17.0.1:5434/k6",
        "dbtable": "NYC",
        "user": "<....>",
        "password": "<......>",
        "driver": "org.postgresql.Driver"}
    
    # Ki·ªÉm tra b·∫£ng PostgreSQL c√≥ t·ªìn t·∫°i kh√¥ng, n·∫øu kh√¥ng th√¨ t·∫°o m·ªõi
    print(f"-----Ki·ªÉm tra b·∫£ng PostgreSQL '{postgres_config['dbtable']}' t·ªìn t·∫°i...------")
    try:
        df_check = spark.read \
            .format("jdbc") \
            .option("url", postgres_config["url"]) \
            .option("dbtable", postgres_config["dbtable"]) \
            .option("user", postgres_config["user"]) \
            .option("password", postgres_config["password"]) \
            .option("driver", postgres_config["driver"]) \
            .load()
        table_exists = True
        print(f"-----B·∫£ng '{postgres_config['dbtable']}' ƒë√£ t·ªìn t·∫°i.------")
    except Exception as e:
        table_exists = False
        print(f"-----B·∫£ng '{postgres_config['dbtable']}' kh√¥ng t·ªìn t·∫°i. S·∫Ω t·∫°o m·ªõi khi ghi d·ªØ li·ªáu.------")
    
    # ========GHI D·ªÆ LI·ªÜU V√ÄO POSTGRESQL==========
    print(f"-------B·∫Øt ƒë·∫ßu Upload d·ªØ li·ªáu l√™n PostgreSQL----------")
    
    try:
        write_mode = "append"  # Lu√¥n append v√¨ ch·ªâ ghi d·ªØ li·ªáu m·ªõi
        
        print(f"  ‚¨ÜÔ∏è  Uploading {row_count} d√≤ng to PostgreSQL...")
        
        df.write \
            .format("jdbc") \
            .option("url", postgres_config["url"]) \
            .option("dbtable", postgres_config["dbtable"]) \
            .option("user", postgres_config["user"]) \
            .option("password", postgres_config["password"]) \
            .option("driver", postgres_config["driver"]) \
            .mode(write_mode) \
            .save()
        
        print(f"-----‚úÖ Upload th√†nh c√¥ng {row_count} d√≤ng-----")
        
        # ===== C·∫¨P NH·∫¨T VERSION TRACKER =====
        with open(version_tracker_file, 'w') as f:
            json.dump({}, f, indent=2)
        
    except Exception as e:
        print(f"  ‚ùå L·ªói khi upload: {e}")
        spark.stop()
        return
    
    spark.stop()
    print("\nüéâ Ho√†n t·∫•t!")

if __name__ == "__main__":
    main()
