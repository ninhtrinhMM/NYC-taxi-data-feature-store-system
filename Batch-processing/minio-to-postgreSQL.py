from pyspark.sql import SparkSession
from delta import configure_spark_with_delta_pip
import os

def main():
    # Spark Config 
    print("-----kh·ªüi t·∫°o Spark Session...------")
    
    builder = (
        SparkSession.builder
        .master("local[16]")
        .appName("MinIO to PostgreSQL")
        
        # Delta Lake configs
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        
        # MinIO/S3 configs
        .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000")  
        .config("spark.hadoop.fs.s3a.access.key", "duongninh")  
        .config("spark.hadoop.fs.s3a.secret.key", "anninhhuyen098")  
        .config("spark.hadoop.fs.s3a.path.style.access", "true")
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")  # N·∫øu MinIO kh√¥ng d√πng SSL

        .config("spark.sql.shuffle.partitions", "16")  # b·∫±ng lu·ªìng CPU
        .config("spark.default.parallelism", "16")  # b·∫±ng lu·ªìng CPU

        .config("spark.executor.memory", "10g")
        .config("spark.driver.memory", "14g")
        .config("spark.driver.maxResultSize", "4g")
        .config("spark.memory.fraction", "0.8") # TƒÉng b·ªô nh·ªõ cho c√°c thao t√°c t√≠nh to√°n

        # OFF-HEAP MEMORY
        .config("spark.memory.offHeap.enabled", "true")
        .config("spark.memory.offHeap.size", "5g")  

        # B·ªï sung JARs cho Hadoop AWS v√† AWS SDK + postgreSQL ƒë·ªÉ ƒë·ªôc v√† ghi tr√™n S3 v√† PostgreSQL
        .config("spark.jars", "/home/ninhtrinhmm/spark-jars/hadoop-aws-3.3.4.jar,/home/ninhtrinhmm/spark-jars/aws-java-sdk-bundle-1.12.262.jar,/home/ninhtrinhmm/spark-jars/postgresql-42.7.1.jar")
    )
    
    spark = configure_spark_with_delta_pip(builder).getOrCreate()
    print("---‚úÖ Spark Session ƒë√£ kh·ªüi t·∫°o th√†nh c√¥ng!---")
    
    # ƒê·ªåC DELTA TABLE T·ª™ MINIO
    
    print(f"-----üì• ƒê·ªçc Delta Table t·ª´ MinIO -----")

    try:
        df = spark.read.format("delta").load("s3a://mlop2/nycdata")
        
        print(f"-----‚úÖ ƒê·ªçc th√†nh c√¥ng! T·ªïng s·ªë d√≤ng: {df.count():,}-----")

    except Exception as e:
        print(f"---‚ùå L·ªói khi ƒë·ªçc t·ª´ MinIO: {e}---")
        spark.stop()
        return
    
    # C·∫§U H√åNH POSTGRESQL CONNECTION

    postgres_config = {
        "url": "jdbc:postgresql://localhost:5434/k6",  # k6 l√† t√™n database
        "dbtable": "NYC",  # Kh·ªüi t·∫°o t√™n table
        "user": "k6",  
        "password": "k6",  
        "driver": "org.postgresql.Driver"
    }

    # ========GHI D·ªÆ LI·ªÜU V√ÄO POSTGRESQL================

    print(f"-----üíæ ƒêang ghi d·ªØ li·ªáu v√†o PostgreSQL table '{postgres_config['dbtable']}'......")
    
    try:
        df.repartition(100).write \
            .format("jdbc") \
            .option("url", postgres_config["url"]) \
            .option("dbtable", postgres_config["dbtable"]) \
            .option("user", postgres_config["user"]) \
            .option("password", postgres_config["password"]) \
            .option("driver", postgres_config["driver"]) \
            .option("batchsize", "1000") \
            .mode("overwrite") \
            .save()
        
        print(f"‚úÖ ƒê√£ ghi th√†nh c√¥ng v√†o PostgreSQL!")
        
    except Exception as e:
        print(f"‚ùå L·ªói khi ghi v√†o PostgreSQL: {e}")
        spark.stop()
        return

    spark.stop()
    print("\nüéâ Ho√†n t·∫•t! Spark Session ƒë√£ ƒë√≥ng.")

if __name__ == "__main__":
    main()